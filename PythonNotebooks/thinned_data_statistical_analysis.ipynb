{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinned Data Statistical Analysis\n",
    "\n",
    "We started collecting about 20x fewer tweets in the middle of 06/25 in order to reduce file size and clumsiness. The original data only captures about 1% of Twitter traffic. We want to test and see if the reduced files are statistically different from the larger data set.\n",
    "\n",
    "In order to do so, we have a few options for statistical tests. Two common tests that we want to run are:\n",
    "1. **ANOVA Test** - Analysis of variance test. This test analyzes the difference between the means of more than two groups.\n",
    "2. **Independent Samples t-test** - This test analyzes the difference between the population means of two groups.\n",
    "\n",
    "The difference between tests exists in how we define a \"group\" of data. It makes sense to define two groups: full data vs. reduced data which suggests a t-test. However, we could create more groups for an ANOVA test by separating by day. Therefore, group 1 could be 06/24 full data, group 2 could be 06/26 reduced data, group 3 - 06/27 reduced data, etc.\n",
    "\n",
    "We want to test a few dependent variables in order to really check for statistical difference. I propose the following set of dependent variables:\n",
    "- ratio of tweets vs. (retweets, quotes, replies)\n",
    "- ratio of retweets vs. (tweets, quotes, replies)\n",
    "- ratio of quotes vs. (tweets, retweets, replies)\n",
    "- ratio of replies vs. (tweets, retweets, quotes)\n",
    "- ratio of common stop words vs. all other words\n",
    "- ratio of selected keywords (Trump, Biden) vs. all other words ratio\n",
    "\n",
    "Our thinned tweets do not include the tweet's timestamp. Therefore, I plan to split each day of data into data points by grouping by a certain size. Ideally I would like > 30 tweets in a chunk to assume normality by the Central Limit Theorem.\n",
    "\n",
    "We can assume independent samples because our data will be examined from different days. Therefore, they will be different tweets. We also have a random sample due to how Twitter allows us to collect data.\n",
    "\n",
    "For the t-test, our null and alternative hypotheses are as follows:\n",
    "$$H_0 : \\mu_1 = \\mu_2$$\n",
    "$$H_1 : \\mu_1 \\neq \\mu_2$$\n",
    "The null states that the population means are equal, and the alternative states that the population means are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These paths should be updated so that no files need to be downloaded...\n",
    "import os\n",
    "import sys\n",
    "shared_path = '/Users/sarah/Downloads/TwitterResearch2020'\n",
    "sys.path.append(shared_path)\n",
    "import thinned_tweet_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From general_utilities.py\n",
    "\n",
    "Basic reading and writing of pickled objects or json objects to/from a file. These functions will not\n",
    "check to see if the file exists (for reading) and will overwrite (when writing). \n",
    "\"\"\"\n",
    "def read_pkl(fname):\n",
    "    \"\"\"\n",
    "    Read a pickled object from a file with path name fname. Returns the object after closing the file.\n",
    "    :param fname: Path name of the file containing the pickle\n",
    "    :return: The object contained in the pickle file\n",
    "    \"\"\"\n",
    "    fout = open(fname, 'rb')\n",
    "    ret_o = pickle.load(fout)\n",
    "    fout.close()\n",
    "    return ret_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data from June, 26 is the first full day of reduced tweets\n",
    "tweets_0626 = read_pkl('combined_tweets-2020-06-26.pkl')\n",
    "len(tweets_0626)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4784443"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data from June, 24 is the larger data set of tweets\n",
    "tweets_0624 = read_pkl('/Users/sarah/Downloads/combined_tweets-2020-06-24.pkl')\n",
    "len(tweets_0624)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ratio(mean_tracker, numerator, denom):\n",
    "    '''\n",
    "    @param mean_tracker: List of means\n",
    "    @param numerator: Numerator for the new mean to calculate\n",
    "    @param denom: Denominator for the new mean to calculate\n",
    "    '''\n",
    "    mean_tracker.append(numerator / denom)\n",
    "    \n",
    "def get_tweet_type_distribution(obj_lst):\n",
    "    '''\n",
    "    Divide a day's worth of combined_tweets into `total_groups` and find the relevant ratios per group \n",
    "    to investigate the distribution of tweet types.\n",
    "    \n",
    "    @param obj_lst: List of combined, thinned tweet objects read from the pkl files\n",
    "    @return: Three lists of length approximately 'total_groups' containing the population means for each \n",
    "    group. The first list contains the ratio of retweets, the second contains the ratio of replies, and \n",
    "    the third contains the ratio of quotes.\n",
    "    '''\n",
    "    lst_len = len(obj_lst)\n",
    "    # One group per minute of the day (we might want to pick a smarter value than this)\n",
    "    total_groups = 60 * 24\n",
    "    group_size = lst_len // total_groups\n",
    "    # 'xxx_mean_tracker' tracks the ratio of xxx vs. [(tweets, retweets, quotes, replies) - xxx]\n",
    "    retweet_mean_tracker = []\n",
    "    reply_mean_tracker = []\n",
    "    quote_mean_tracker = []\n",
    "    \n",
    "    tweet_count = 0\n",
    "    retweet_count = 0\n",
    "    reply_count = 0\n",
    "    quote_count = 0\n",
    "    for thin_obj in obj_lst:\n",
    "        if tweet_count == group_size:\n",
    "            append_ratio(retweet_mean_tracker, retweet_count, tweet_count)\n",
    "            append_ratio(reply_mean_tracker, reply_count, tweet_count)\n",
    "            append_ratio(quote_mean_tracker, quote_count, tweet_count)\n",
    "            \n",
    "            tweet_count = 0\n",
    "            retweet_count = 0\n",
    "            reply_count = 0\n",
    "            quote_count = 0\n",
    "        \n",
    "        tweet_count += 1\n",
    "        if thin_obj.is_retweet: retweet_count += 1\n",
    "        if thin_obj.is_reply(): reply_count += 1\n",
    "        if thin_obj.quote_status: quote_count += 1\n",
    "            \n",
    "    return retweet_mean_tracker, reply_mean_tracker, quote_mean_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_stats(obj_lst):\n",
    "        '''\n",
    "    Print the percentage of retweets, replies, and quotes per one day of combined_tweets.\n",
    "    \n",
    "    @param obj_lst: List of combined, thinned tweet objects read from the pkl files\n",
    "    @return: Three lists of length approximately 'total_groups' containing the population means for each \n",
    "    group. The first list contains the ratio of retweets, the second contains the ratio of replies, and \n",
    "    the third contains the ratio of quotes.\n",
    "    '''\n",
    "    retweet_means, reply_means, quote_means = get_tweet_type_distribution(obj_lst)\n",
    "    print(\"percentage retweets: \", np.array(retweet_means).mean())\n",
    "    print(\"percentage replies: \", np.array(reply_means).mean())\n",
    "    print(\"percentage quotes: \", np.array(quote_means).mean())\n",
    "    return retweet_means, reply_means, quote_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Thinned data stats -----\n",
      "percentage retweets:  0.6245257029991164\n",
      "percentage replies:  1.0\n",
      "percentage quotes:  0.2569520245334997\n",
      "----- Full data stats -----\n",
      "percentage retweets:  0.6217230249515018\n",
      "percentage replies:  1.0\n",
      "percentage quotes:  0.25006124991638234\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Thinned data stats -----\")\n",
    "retweet_means_thin, reply_means_thin, quote_means_thin = get_dist_stats(tweets_0626)\n",
    "\n",
    "print(\"----- Full data stats -----\")\n",
    "retweet_means_full, reply_means_full, quote_means_full = get_dist_stats(tweets_0624)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There appears to be an error with the `is_reply` field.\n",
    "\n",
    "In the future, we may want to consider different ways of constructing groups within a day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f\n",
    "\n",
    "def t_test(data_lst_thin, data_lst_full):\n",
    "    '''\n",
    "    Runs a t-test to compare the population means between the reduced and larger combined_tweet data.\n",
    "    \n",
    "    @param data_lst_thin: List of population means from the reduced tweet data.\n",
    "    @param data_lst_full: List of population means from the larger tweet data.\n",
    "    '''\n",
    "    N = min(len(data_lst_thin), len(data_lst_full))\n",
    "    data_lst_thin = np.array(data_lst_thin[:N])\n",
    "    data_lst_full = np.array(data_lst_full[:N])\n",
    "    var_thin = data_lst_thin.var(ddof=1)\n",
    "    var_full = data_lst_full.var(ddof=1)\n",
    "    st_dev = np.sqrt((var_thin + var_full) / 2)\n",
    "    t_stat = (data_lst_thin.mean() - data_lst_full.mean()) / (st_dev * np.sqrt(2 / N))\n",
    "    # Degrees of freedom\n",
    "    df = 2 * N - 2\n",
    "    # p-value after comparison with the Student t distribution\n",
    "    p = 1 - stats.t.cdf(t_stat, df=df)\n",
    "    \n",
    "    print(\"t_stat = \" + str(t_stat))\n",
    "    # Reject the null if the p-value is < alpha (0.05)\n",
    "    print(\"p_val = \" + str(2 * p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- retweets t-test -----\n",
      "t_stat = 1.1778701790635677\n",
      "p_val = 0.2389457525069174\n",
      "----- replies t-test -----\n",
      "t_stat = -419.45586433151715\n",
      "p_val = 2.0\n",
      "----- quotes t-test -----\n",
      "t_stat = 3.5594451365156825\n",
      "p_val = 0.0003776509718413923\n"
     ]
    }
   ],
   "source": [
    "print(\"----- retweets t-test -----\")\n",
    "t_test(retweet_means_thin, retweet_means_full)\n",
    "print(\"----- replies t-test -----\")\n",
    "t_test(quote_means_thin, reply_means_full)\n",
    "print(\"----- quotes t-test -----\")\n",
    "t_test(quote_means_thin, quote_means_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to throw replies out of our analysis because of the error in the `is_reply` field. Otherwise, we can conclude that the ratio of retweets are not statistically different between the reduced and larger data. We do reject the null for the ratio of quotes, instead, concluding that the ratio of quotes *are* statistically different between the reduced and larger data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try re-running the t-test's with fewer subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_subgroups_2x(lst):\n",
    "    lst_merged = []\n",
    "    i = 0\n",
    "    s = 0\n",
    "    for val in lst:\n",
    "        if i == 1:\n",
    "            s += val\n",
    "            lst_merged.append(s / 2)\n",
    "            s = 0\n",
    "            i = 0\n",
    "        else:\n",
    "            s += val\n",
    "            i += 1\n",
    "            \n",
    "    return lst_merged\n",
    "\n",
    "def reduce_subgroups(lst, factor):\n",
    "    for i in range(factor):\n",
    "        lst = reduce_subgroups_2x(lst)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "def run_t_test_reduced(factor):\n",
    "    print(\"Number of groups:\", len(reduce_subgroups(retweet_means_thin, factor)))\n",
    "    print()\n",
    "    print(\"----- retweets t-test -----\")\n",
    "    t_test(reduce_subgroups(retweet_means_thin, factor), reduce_subgroups(retweet_means_full, factor))\n",
    "    print(\"----- replies t-test -----\")\n",
    "    t_test(reduce_subgroups(quote_means_thin, factor), reduce_subgroups(reply_means_full, factor))\n",
    "    print(\"----- quotes t-test -----\")\n",
    "    t_test(reduce_subgroups(quote_means_thin, factor), reduce_subgroups(quote_means_full, factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 363\n",
      "\n",
      "----- retweets t-test -----\n",
      "t_stat = 0.7022445391933957\n",
      "p_val = 0.4827542601214765\n",
      "----- replies t-test -----\n",
      "t_stat = -251.25316023753558\n",
      "p_val = 2.0\n",
      "----- quotes t-test -----\n",
      "t_stat = 2.049535521920288\n",
      "p_val = 0.04077258635862613\n",
      "\n",
      "Number of groups: 181\n",
      "\n",
      "----- retweets t-test -----\n",
      "t_stat = 0.5171379470537968\n",
      "p_val = 0.6053792140263239\n",
      "----- replies t-test -----\n",
      "t_stat = -186.4929970888013\n",
      "p_val = 2.0\n",
      "----- quotes t-test -----\n",
      "t_stat = 1.5058637869152451\n",
      "p_val = 0.13298394062677543\n",
      "\n",
      "Number of groups: 45\n",
      "\n",
      "----- retweets t-test -----\n",
      "t_stat = 0.2907302998655875\n",
      "p_val = 0.7719416746731556\n",
      "----- replies t-test -----\n",
      "t_stat = -106.22773289858971\n",
      "p_val = 2.0\n",
      "----- quotes t-test -----\n",
      "t_stat = 0.8432925927446651\n",
      "p_val = 0.4013512144672402\n"
     ]
    }
   ],
   "source": [
    "run_t_test_reduced(2)\n",
    "print()\n",
    "run_t_test_reduced(3)\n",
    "print()\n",
    "run_t_test_reduced(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with groups = 181, we can accept the null in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of words in tweet\n",
    "\n",
    "Let's look at the actual words in a tweet to determine if the data is statistically different. First, we will examine the ratio of stopwords. Stopwords are English words that occur frequently such as 'the,' 'and,' 'a,' etc. We would like to see a similar ratio of stopwords between the reduced and larger data because this can help us conclude that the actual tweets are statistically similar.\n",
    "\n",
    "We will also examine how many times a Trump or Biden related word appears as well as the ratio of tweets that mention Trump or Biden. While the number of times a Trump or Biden word may not be a great indicator of statistical similarity, the number of tweets that mention either candidate may be useful. This measure will allow us to glance at the virality of each candidate and will help us determine if the data that we wish to study about the tweets is statistically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common english words (ex. 'the,' 'and,' 'a,' etc.)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ratios(obj_lst):\n",
    "    '''\n",
    "    Divide a day's worth of combined_tweets into `total_groups` and find the relevant ratios per group \n",
    "    to investigate the distribution of tweet types.\n",
    "    \n",
    "    @param obj_lst: List of combined, thinned tweet objects read from the pkl files\n",
    "    @return: Three lists of length approximately 'total_groups' containing the population means for each \n",
    "    group. The first list contains the ratio of stopwords, the second contains the ratio of trump-related words, and \n",
    "    the third contains the ratio of biden-related words.\n",
    "    '''\n",
    "    lst_len = len(obj_lst)\n",
    "    total_groups = 60 * 24\n",
    "    group_size = lst_len // total_groups\n",
    "    stopword_mean_tracker = []\n",
    "    trump_words_mean_tracker = []\n",
    "    trump_tweet_mean_tracker = []\n",
    "    biden_words_mean_tracker = []\n",
    "    biden_tweet_mean_tracker = []\n",
    "\n",
    "    stopword_counter = 0\n",
    "    trump_counter = 0\n",
    "    trump_related_tweet_counter = 0\n",
    "    biden_counter = 0\n",
    "    biden_related_tweet_counter = 0\n",
    "    tweet_word_count = 0\n",
    "    tweet_count = 0\n",
    "    for thin_obj in obj_lst:\n",
    "        if tweet_count == group_size:\n",
    "            append_ratio(stopword_mean_tracker, stopword_counter, tweet_word_count)\n",
    "            append_ratio(trump_words_mean_tracker, trump_counter, tweet_word_count)\n",
    "            append_ratio(biden_words_mean_tracker, biden_counter, tweet_word_count)\n",
    "            append_ratio(trump_tweet_mean_tracker, trump_related_tweet_counter, tweet_count)\n",
    "            append_ratio(biden_tweet_mean_tracker, biden_related_tweet_counter, tweet_count)\n",
    "            \n",
    "            tweet_count = 0\n",
    "            tweet_word_count = 0\n",
    "            stopword_counter = 0\n",
    "            trump_counter = 0\n",
    "            trump_related_tweet_counter = 0\n",
    "            biden_counter = 0\n",
    "            biden_related_tweet_counter = 0\n",
    "            \n",
    "        tweet_count += 1\n",
    "        if thin_obj.is_retweet: thin_obj = thin_obj.retweet\n",
    "        word_tokens = word_tokenize(thin_obj.text)\n",
    "        tweet_word_count += len(word_tokens)\n",
    "        \n",
    "        trump_related_tweet_flag = False\n",
    "        biden_related_tweet_flag = False\n",
    "        for w in word_tokens:\n",
    "            w = w.lower()\n",
    "            if w in stop_words: stopword_counter += 1\n",
    "            if w in ['trump', 'realdonaldtrump', '@realdonaldtrump']: \n",
    "                trump_counter += 1\n",
    "                if not trump_related_tweet_flag:\n",
    "                    trump_related_tweet_flag = True\n",
    "                    trump_related_tweet_counter += 1\n",
    "            if w in ['biden', 'joebiden', '@joebiden']: \n",
    "                biden_counter += 1\n",
    "                if not biden_related_tweet_flag:\n",
    "                    biden_related_tweet_flag = True\n",
    "                    biden_related_tweet_counter += 1\n",
    "                \n",
    "    return stopword_mean_tracker, trump_words_mean_tracker, trump_tweet_mean_tracker, biden_words_mean_tracker, biden_tweet_mean_tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_stats(obj_lst):\n",
    "    stopword_mean_tracker, trump_mean_tracker, trump_tweet_mean_tracker, biden_mean_tracker, biden_tweet_mean_tracker = get_word_ratios(obj_lst)\n",
    "    print(\"percentage of words that are stopwords: \", np.array(stopword_mean_tracker).mean())\n",
    "    print()\n",
    "    print(\"percentage of words that are trump related: \", np.array(trump_mean_tracker).mean())\n",
    "    print(\"percentage of tweets that mention trump: \", np.array(trump_tweet_mean_tracker).mean())\n",
    "    print()\n",
    "    print(\"percentage of words that are biden related: \", np.array(biden_mean_tracker).mean())\n",
    "    print(\"percentage of tweets that mention biden: \", np.array(biden_tweet_mean_tracker).mean())\n",
    "    return stopword_mean_tracker, trump_mean_tracker, trump_tweet_mean_tracker, biden_mean_tracker, biden_tweet_mean_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Thinned data stats -----\n",
      "percentage of words that are stopwords:  0.27571611620491726\n",
      "\n",
      "percentage of words that are trump related:  0.0077525441164554764\n",
      "percentage of tweets that mention trump:  0.15056655751338427\n",
      "\n",
      "percentage of words that are biden related:  0.001802221817679685\n",
      "percentage of tweets that mention biden:  0.03489006705130204\n",
      "----- Full data stats -----\n",
      "percentage of words that are stopwords:  0.2882371591545534\n",
      "\n",
      "percentage of words that are trump related:  0.006473343776062667\n",
      "percentage of tweets that mention trump:  0.12229915044484581\n",
      "\n",
      "percentage of words that are biden related:  0.0014778323257913111\n",
      "percentage of tweets that mention biden:  0.028117474413004218\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Thinned data stats -----\")\n",
    "stopword_means_thin, trump_means_thin, trump_mentions_means_thin, biden_means_thin, biden_mentions_means_thin = get_word_stats(tweets_0626)\n",
    "\n",
    "print(\"----- Full data stats -----\")\n",
    "stopword_means_full, trump_means_full, trump_mentions_means_full, biden_means_full, biden_mentions_means_full = get_word_stats(tweets_0624)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- stopwords t-test -----\n",
      "t_stat = -10.29123494922213\n",
      "p_val = 2.0\n",
      "\n",
      "----- trump words t-test -----\n",
      "t_stat = 7.423273895486341\n",
      "p_val = 1.496580637194711e-13\n",
      "----- tweets that mention trump t-test -----\n",
      "t_stat = 8.589418849462756\n",
      "p_val = 0.0\n",
      "\n",
      "----- biden words t-test -----\n",
      "t_stat = 6.084520959546995\n",
      "p_val = 1.3231316042805474e-09\n",
      "----- tweets that mention biden t-test -----\n",
      "t_stat = 6.831629876800464\n",
      "p_val = 1.0204725953144589e-11\n"
     ]
    }
   ],
   "source": [
    "print(\"----- stopwords t-test -----\")\n",
    "t_test(stopword_means_thin, stopword_means_full)\n",
    "print()\n",
    "print(\"----- trump words t-test -----\")\n",
    "t_test(trump_means_thin, trump_means_full)\n",
    "print(\"----- tweets that mention trump t-test -----\")\n",
    "t_test(trump_mentions_means_thin, trump_mentions_means_full)\n",
    "print()\n",
    "print(\"----- biden words t-test -----\")\n",
    "t_test(biden_means_thin, biden_means_full)\n",
    "print(\"----- tweets that mention biden t-test -----\")\n",
    "t_test(biden_mentions_means_thin, biden_mentions_means_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the ratio of stopwords are not statistically different between the reduced and larger data. This may be a good metric to determine that the tweets themselves are not statistically different. \n",
    "\n",
    "However, notice that we reject the null for the ratio of times trump related words appear and tweets that mention trump, concluding that these *are* statistically different between the reduced and larger data. We also conclude that the ratio of times biden related words appear and the ratio of tweets that mention biden are statistically different between the reduced and larger data. \n",
    "\n",
    "If we reduce the amount of groups (increasing the number of tweets per group), we do not reject the null and can then claim that the data is not statistically different. This raises a question on how we wish to determine subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_words_t_test_reduced(factor):\n",
    "    print(\"Number of groups:\", len(reduce_subgroups(trump_means_thin, factor)))\n",
    "    print()\n",
    "    print(\"----- stopwords t-test -----\")\n",
    "    t_test(reduce_subgroups(stopword_means_thin, factor), reduce_subgroups(stopword_means_full, factor))\n",
    "    print()\n",
    "    print(\"----- trump words t-test -----\")\n",
    "    t_test(reduce_subgroups(trump_means_thin, factor), reduce_subgroups(trump_means_full, factor))\n",
    "    print(\"----- tweets that mention trump t-test -----\")\n",
    "    t_test(reduce_subgroups(trump_mentions_means_thin, factor), reduce_subgroups(trump_mentions_means_full, factor))\n",
    "    print()\n",
    "    print(\"----- biden words t-test -----\")\n",
    "    t_test(reduce_subgroups(biden_means_thin, factor), reduce_subgroups(biden_means_full, factor))\n",
    "    print(\"----- tweets that mention biden t-test -----\")\n",
    "    t_test(reduce_subgroups(biden_mentions_means_thin, factor), reduce_subgroups(biden_mentions_means_full, factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 181\n",
      "\n",
      "----- stopwords t-test -----\n",
      "t_stat = -3.8327474871784104\n",
      "p_val = 1.999850344544159\n",
      "\n",
      "----- trump words t-test -----\n",
      "t_stat = 2.755516145136178\n",
      "p_val = 0.006159620609314409\n",
      "----- tweets that mention trump t-test -----\n",
      "t_stat = 3.1831926196199594\n",
      "p_val = 0.0015841691150355608\n",
      "\n",
      "----- biden words t-test -----\n",
      "t_stat = 2.580276291112289\n",
      "p_val = 0.010269836394463328\n",
      "----- tweets that mention biden t-test -----\n",
      "t_stat = 2.9257240900210904\n",
      "p_val = 0.0036557635217886464\n",
      "\n",
      "Number of groups: 90\n",
      "\n",
      "----- stopwords t-test -----\n",
      "t_stat = -2.7420306880134615\n",
      "p_val = 1.9932701271338433\n",
      "\n",
      "----- trump words t-test -----\n",
      "t_stat = 1.9821914288366276\n",
      "p_val = 0.0489967400614415\n",
      "----- tweets that mention trump t-test -----\n",
      "t_stat = 2.2857990179052283\n",
      "p_val = 0.023444685488855033\n",
      "\n",
      "----- biden words t-test -----\n",
      "t_stat = 1.900846992229877\n",
      "p_val = 0.058938029453291296\n",
      "----- tweets that mention biden t-test -----\n",
      "t_stat = 2.1542933064411227\n",
      "p_val = 0.032561541012167794\n",
      "\n",
      "Number of groups: 45\n",
      "\n",
      "----- stopwords t-test -----\n",
      "t_stat = -1.9924930266312282\n",
      "p_val = 1.9505839422064115\n",
      "\n",
      "----- trump words t-test -----\n",
      "t_stat = 1.4299706658874156\n",
      "p_val = 0.15626634269691642\n",
      "----- tweets that mention trump t-test -----\n",
      "t_stat = 1.6462436374244709\n",
      "p_val = 0.10328061852236092\n",
      "\n",
      "----- biden words t-test -----\n",
      "t_stat = 1.4708744990998002\n",
      "p_val = 0.14489188967359046\n",
      "----- tweets that mention biden t-test -----\n",
      "t_stat = 1.6681309927757346\n",
      "p_val = 0.09884440330060662\n"
     ]
    }
   ],
   "source": [
    "run_words_t_test_reduced(3)\n",
    "print()\n",
    "run_words_t_test_reduced(4)\n",
    "print()\n",
    "run_words_t_test_reduced(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with groups = 45, we can accept the null in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After performing several t-tests to compare the reduced and larger data sets of combined, thinned tweets, we can make several conclusions. First, we can accept the null hypothesis and conclude that the data sets are not statistically different for the ratio of retweets. We must reject this conclusion for the ratio of quotes. We cannot make any conclusion for the ratio of replies due to an error with the `is_reply` field. It would be worthwhile to rerun the t-tests with a more thoughtful choice for subgroups.\n",
    "\n",
    "The ratio of stopwords are not statistically different between the reduced and larger data. This may be a good metric to determine that the tweets themselves are not statistically different since stopwords are common words between English statements. \n",
    "\n",
    "However, using the current subgroup size of 1440, we reject the null for all other scenarios, concluding that the tweets *are* statistically different between the reduced and larger data. If we reduce the amount of groups (increasing the number of tweets per group), we do not reject the null and can then claim that the data is not statistically different. This raises a question on how we wish to determine subgroups.\n",
    "\n",
    "It is a little surprisingly to see the lack of mentions to Trump and Biden. Even more so, Biden is mentioned 5x as frequently as Trump. This raises questions on how we can determine whether a tweet is about Trump or Biden. We should establish a robust criteria and re-run the above t-tests. \n",
    "\n",
    "**Future considerations:**\n",
    "- Determine a smarter criteria for subgroup size\n",
    "- Establish a robust criteria to determine a Trump or Biden related tweet\n",
    "- Model the retweet vs. reply vs. quote as a multinomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
